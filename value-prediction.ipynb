{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nimport lightgbm\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LogisticRegression\n\nfrom lightgbm import LGBMClassifier\nimport csv\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, RandomizedSearchCV, GridSearchCV\nimport xgboost as xgb\n# data normalization with sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRegressor\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/player-20-clean/player_20_clean.csv', index_col = False)\nval_nl = df[df['value_eur']==0].index.tolist()\ndf.drop(df.index[val_nl],inplace= True)\ny = df['value_eur'].values\ndf.drop(['value_eur'],axis = 1 , inplace = True)\nx = df.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/my-fifa-19/final_data_v3.csv', index_col = False)\n# pos_nl = df[df['Value']==0].index.tolist()\n# print ('Number of null positions: ', len(pos_nl))\n# df.drop(df.index[pos_nl],inplace= True)\npos_nl = df[df['LS']==0].index.tolist()\nval_nl = df[df['Value']==0].index.tolist()\ntotal_nl = list(set(pos_nl + val_nl))\nprint ('Number of Goalkeepers ', len(pos_nl))\ndf.drop(df.index[total_nl],inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Value'].values\ndf.drop(['Value', 'Jersey Number', 'Joined','Contract Valid Until', 'Preferred Foot'], inplace=True, axis=1)\n#df.drop(['Value'], axis = 1 , inplace = True)\nx = df.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df  = pd.read_csv('/kaggle/input/data-42/data_42.csv', index_col = False)\nval_nl = df[df['Value']==0].index.tolist()\nprint ('Number of Goalkeepers ', len(pos_nl))\ndf.drop(df.index[val_nl],inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y = df['Value'].values\ndf.drop(['Value'], axis = 1 , inplace = True)\nx = df.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CustomTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, Columns):\n        self.Columns=Columns\n        \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        New_X=X.copy()\n        New_X=New_X[self.Columns].copy()\n        return New_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\npipeline=Pipeline([\n    ('custom_tr', CustomTransformer(Skill_cols)),\n    ('imputer',SimpleImputer(strategy='median')),\n    ('std_scaler',StandardScaler())\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=pipeline.fit_transform(df)\ny=df['value_eur'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.252, random_state=2)\n\nprint (X_train.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_fold(X, Y, clf):\n    #  shuffle dataset randomly , split the dataset into 10 groups\n    kf = KFold(n_splits=10, random_state=1, shuffle=True)\n\n    rmses= []\n    rmses_lg= []\n    #Y=  Y/100000\n    yp = []\n    yt = []\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        Y_train, Y_test = Y[train_index], Y[test_index]\n        # data normalization with sklearn\n        X_train = MinMaxScaler().fit_transform(X_train)\n        #Y_train = MinMaxScaler().fit_transform(Y_train.reshape(-1,1))\n        Y_train = np.log(Y_train)\n        \n        #print (X_train)\n        clf.fit(X_train, Y_train.ravel())\n        \n        X_test= MinMaxScaler().fit_transform(X_test)\n        #norm_test = MinMaxScaler().fit(Y_test.reshape(-1, 1))\n        #Y_test = norm_test.transform(Y_test.reshape(-1, 1))\n        Y_pred = clf.predict(X_test)\n        Y_test_lg = np.log (Y_test)\n        #yp = np.rint(Y_pred)\n        #yt = Y_test\n        rmse_lg = mean_squared_error(Y_test_lg,Y_pred) ** 0.5\n        print('The rmse of prediction is:', rmse_lg)\n        rmses_lg.append(rmse_lg)\n        \n        rmse = mean_squared_error(Y_test,np.exp(Y_pred)) ** 0.5\n        print('The rmse of prediction is:', rmse)\n        rmses.append(rmse)\n    \n    average_rmse = np.mean(np.array(rmses))\n    average_rmse_lg = np.mean(np.array(rmses_lg))\n    #yt = norm_test.inverse_transform(Y_test)\n    #yp = norm_test.inverse_transform(Y_pred.reshape(-1,1))\n    return clf, average_rmse,average_rmse_lg,Y_pred,  Y_test_lg\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_fold_scl(X, Y, clf):\n    #  shuffle dataset randomly , split the dataset into 10 groups\n    kf = KFold(n_splits=10, random_state=1, shuffle=True)\n\n    rmses= []\n    rmses_scl = []\n    Y=  Y/1000000\n    yp = []\n    yt = []\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        Y_train, Y_test = Y[train_index], Y[test_index]\n        # data normalization with sklearn\n        X_train = MinMaxScaler().fit_transform(X_train)\n        \n        #print (X_train)\n        clf.fit(X_train, Y_train.ravel())\n        \n        X_test= MinMaxScaler().fit_transform(X_test)\n        Y_pred = clf.predict(X_test)\n        rmse = np.sqrt(mean_squared_error(Y_test,Y_pred))\n        rmse_scl = mean_squared_error(Y_test*1000000,Y_pred*10000000) ** 0.5\n        print('The rmse of prediction is:', rmse)\n        rmses.append(rmse)\n        rmses_scl.append(rmse_scl)\n        \n        \n    \n    avg_rmse = np.mean(np.array(rmses))\n    avg_rmse_scl = np.mean(np.array(rmses_scl))\n    return avg_rmse,avg_rmse_scl,  Y_pred,Y_test\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_predict(clf, X_test, Y_test):\n        X_test= MinMaxScaler().fit_transform(X_test)\n        Y_pred = clf.predict(X_test)\n        Y_test_lg = np.log (Y_test)\n\n        rmse_lg = mean_squared_error(Y_test_lg,Y_pred) ** 0.5\n        print('The rmse of prediction is:', rmse_lg)\n        \n        rmse = mean_squared_error(Y_test,np.exp(Y_pred)) ** 0.5\n        print('The rmse of prediction is:', rmse)\n        #rmses.append(rmse)\n        return rmse_lg, rmse , Y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_lr = LinearRegression()\nclf_lgbm = lightgbm.LGBMRegressor(boosting_type=  'gbdt', learning_rate =  0.1, num_leaves =  31)\nclf_xgb = XGBRegressor(booster =  'gbtree', learning_rate =  0.1)\nclf_cat = CatBoostRegressor(iterations = 100, learning_rate = 0.1)\nclf_rf = RandomForestRegressor(max_features= 10, n_estimators= 30)\nmodel_names = list(['lr', 'rfr', 'lgbmr', 'xgbr', 'catbr'])\nmodels = list([clf_lr,  clf_rf, clf_lgbm , clf_xgb, clf_cat])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = []\nprd_list = []\nfor idx, model in enumerate(models) :\n    X_train_cp= np.copy(X_train)\n    y_train_cp= np.copy(y_train)\n    print (model_names[idx])\n    fit_model, rmse, rmsel, ypl, ytl = k_fold(X_train,y_train,model)\n    trmsel, trmse, yp = model_predict (fit_model, np.copy(X_test), np.copy(y_test))\n    result.append(list([model_names[idx]]) + list([rmse, trmse, rmsel, trmsel]))\n    prd_list.append(yp)\nodf = pd.DataFrame(result, columns = list(['model_name', 'rmse', 'trmse','rmsel','trmsel'] ))\n#fdf.append()\nodf.to_csv('original_dataset_result.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fdf = odf.copy()\nfdf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"42 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fdf_42 = odf.copy()\nfdf_42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prd_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All Features"},{"metadata":{},"cell_type":"markdown","source":"Logarithmic Scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, prd in enumerate(prd_list):\n    # print ()\n    ypl = prd\n    ytl = np.copy(np.log(y_test))\n    plt.rcParams['figure.figsize'] = [20, 5]\n    fig, ax = plt.subplots()\n    ax.scatter(ytl, ypl)\n    ax.plot([ytl.min(), ytl.max()], [ytl.min(), ytl.max()], 'k--', lw=4)\n    ax.set_xlabel(model_names[idx] + ' Measured')\n    ax.set_ylabel('Predicted')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normal Scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, prd in enumerate(prd_list):\n    # print ()\n    ypl = np.exp(prd)\n    ytl = np.copy(y_test)\n    plt.rcParams['figure.figsize'] = [20, 5]\n    fig, ax = plt.subplots()\n    ax.scatter(ytl, ypl)\n    ax.plot([ytl.min(), ytl.max()], [ytl.min(), ytl.max()], 'k--', lw=4)\n    ax.set_xlabel(model_names[idx] + ' Measured')\n    ax.set_ylabel('Predicted')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"42 features"},{"metadata":{},"cell_type":"markdown","source":"Logarithmic scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, prd in enumerate(prd_list):\n    # print ()\n    ypl = prd\n    ytl = np.copy(np.log(y_test))\n    plt.rcParams['figure.figsize'] = [20, 5]\n    fig, ax = plt.subplots()\n    ax.scatter(ytl, ypl)\n    ax.plot([ytl.min(), ytl.max()], [ytl.min(), ytl.max()], 'k--', lw=4)\n    ax.set_xlabel(model_names[idx] + ' Measured')\n    ax.set_ylabel('Predicted')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normal Scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, prd in enumerate(prd_list):\n    # print ()\n    ypl = np.exp(prd)\n    ytl = np.copy(y_test)\n    plt.rcParams['figure.figsize'] = [20, 5]\n    fig, ax = plt.subplots()\n    ax.scatter(ytl, ypl)\n    ax.plot([ytl.min(), ytl.max()], [ytl.min(), ytl.max()], 'k--', lw=4)\n    ax.set_xlabel(model_names[idx] + ' Measured')\n    ax.set_ylabel('Predicted')\n    plt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}